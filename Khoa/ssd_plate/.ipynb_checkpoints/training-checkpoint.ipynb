{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# from models.keras_ssd512 import ssd_512\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 1 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\My_Work\\Python\\ssd_plate\\keras_loss_function\\keras_ssd_loss.py:133: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\My_Work\\Python\\ssd_plate\\keras_loss_function\\keras_ssd_loss.py:166: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# 1: Build the Keras model.\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "# model = ssd_512(image_size=(img_height, img_width, 3),\n",
    "#                 n_classes=20,\n",
    "#                 mode='inference',\n",
    "#                 l2_regularization=0.0005,\n",
    "#                 scales=[0.07, 0.15, 0.3, 0.45, 0.6, 0.75, 0.9, 1.05], # The scales for MS COCO are [0.04, 0.1, 0.26, 0.42, 0.58, 0.74, 0.9, 1.06]\n",
    "#                 aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "#                                          [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "#                                          [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "#                                          [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "#                                          [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "#                                          [1.0, 2.0, 0.5],\n",
    "#                                          [1.0, 2.0, 0.5]],\n",
    "#                two_boxes_for_ar1=True,\n",
    "#                steps=[8, 16, 32, 64, 128, 256, 512],\n",
    "#                offsets=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "#                clip_boxes=False,\n",
    "#                variances=[0.1, 0.1, 0.2, 0.2],\n",
    "#                normalize_coords=True,\n",
    "#                subtract_mean=[123, 117, 104],\n",
    "#                swap_channels=[2, 1, 0],\n",
    "#                confidence_thresh=0.5,\n",
    "#                iou_threshold=0.45,\n",
    "#                top_k=200,\n",
    "#                nms_max_output_size=400)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "\n",
    "# TODO: Set the path to the weights you want to load.\n",
    "weights_path = 'VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "# run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "# adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "# model.compile(optimizer=sgd, loss=ssd_loss.compute_loss, metrics=['accuracy'])\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'train.txt': 100%|██████████| 1647/1647 [00:01<00:00, 1003.25it/s]\n",
      "Creating HDF5 dataset: 100%|██████████| 1647/1647 [00:10<00:00, 150.07it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "images_dir      = 'GreenParking/'\n",
    "annotations_dir      = 'GreenParking/'\n",
    "train_image_set_filename    = 'GreenParking/train.txt'\n",
    "\n",
    "classes = ['plate']\n",
    "\n",
    "\n",
    "train_dataset.parse_xml(images_dirs=[images_dir],\n",
    "                        image_set_filenames=[train_image_set_filename],\n",
    "                        annotations_dirs=[annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "\n",
    "train_dataset.create_hdf5_dataset(file_path='dataset_plate_trainval.h5',\n",
    "                                  resize=False,\n",
    "                                  variable_image_size=True,\n",
    "                                  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "Number of images in the training dataset:\t  1647\n"
     ]
    }
   ],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 8 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "print(n_classes)\n",
    "print(len(classes))\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule.\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 80:\n",
    "        return 0.001\n",
    "    elif epoch < 100:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model callbacks.\n",
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='ssd300_plate_epoch-{epoch:02d}_loss-{loss:.4f}_acc-{acc:.3f}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "#model_checkpoint.best = \n",
    "\n",
    "csv_logger = CSVLogger(filename='ssd300_plate_training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 204/1000 [=====>........................] - ETA: 5:06:32 - loss: 2.8816 - acc: 0.08 - ETA: 2:38:56 - loss: 2.8816 - acc: 0.09 - ETA: 1:49:34 - loss: 2.8816 - acc: 0.08 - ETA: 1:24:54 - loss: 2.8816 - acc: 0.08 - ETA: 1:10:06 - loss: 2.8816 - acc: 0.08 - ETA: 1:00:13 - loss: 2.8816 - acc: 0.08 - ETA: 53:09 - loss: 2.8816 - acc: 0.0845 - ETA: 47:52 - loss: 2.8816 - acc: 0.08 - ETA: 43:46 - loss: 2.8816 - acc: 0.08 - ETA: 40:28 - loss: 2.8815 - acc: 0.08 - ETA: 37:47 - loss: 2.8815 - acc: 0.08 - ETA: 35:32 - loss: 2.8815 - acc: 0.08 - ETA: 33:38 - loss: 2.8815 - acc: 0.08 - ETA: 32:00 - loss: 2.8815 - acc: 0.08 - ETA: 30:35 - loss: 2.8815 - acc: 0.08 - ETA: 29:21 - loss: 2.8814 - acc: 0.08 - ETA: 28:15 - loss: 2.8814 - acc: 0.08 - ETA: 27:16 - loss: 2.8814 - acc: 0.08 - ETA: 26:23 - loss: 2.8814 - acc: 0.08 - ETA: 25:35 - loss: 2.8814 - acc: 0.08 - ETA: 24:52 - loss: 2.8813 - acc: 0.08 - ETA: 24:13 - loss: 2.8813 - acc: 0.08 - ETA: 23:37 - loss: 2.8813 - acc: 0.08 - ETA: 23:04 - loss: 2.8813 - acc: 0.08 - ETA: 22:34 - loss: 2.8813 - acc: 0.08 - ETA: 22:05 - loss: 2.8812 - acc: 0.08 - ETA: 21:39 - loss: 2.8812 - acc: 0.08 - ETA: 21:15 - loss: 2.8812 - acc: 0.08 - ETA: 20:53 - loss: 2.8812 - acc: 0.08 - ETA: 20:32 - loss: 2.8811 - acc: 0.08 - ETA: 20:12 - loss: 2.8811 - acc: 0.08 - ETA: 19:53 - loss: 2.8811 - acc: 0.08 - ETA: 19:36 - loss: 2.8811 - acc: 0.08 - ETA: 19:19 - loss: 2.8810 - acc: 0.08 - ETA: 19:04 - loss: 2.8810 - acc: 0.08 - ETA: 18:49 - loss: 2.8810 - acc: 0.08 - ETA: 18:35 - loss: 2.8810 - acc: 0.08 - ETA: 18:22 - loss: 2.8809 - acc: 0.08 - ETA: 18:10 - loss: 2.8809 - acc: 0.08 - ETA: 17:58 - loss: 2.8809 - acc: 0.08 - ETA: 17:47 - loss: 2.8809 - acc: 0.08 - ETA: 17:36 - loss: 2.8808 - acc: 0.08 - ETA: 17:26 - loss: 2.8808 - acc: 0.08 - ETA: 17:16 - loss: 2.8808 - acc: 0.08 - ETA: 17:06 - loss: 2.8808 - acc: 0.08 - ETA: 16:57 - loss: 2.8807 - acc: 0.08 - ETA: 16:48 - loss: 2.8807 - acc: 0.08 - ETA: 16:39 - loss: 2.8807 - acc: 0.08 - ETA: 16:31 - loss: 2.8806 - acc: 0.08 - ETA: 16:23 - loss: 2.8806 - acc: 0.08 - ETA: 16:16 - loss: 2.8806 - acc: 0.08 - ETA: 16:08 - loss: 2.8806 - acc: 0.08 - ETA: 16:01 - loss: 2.8805 - acc: 0.08 - ETA: 15:54 - loss: 2.8805 - acc: 0.08 - ETA: 15:48 - loss: 2.8805 - acc: 0.08 - ETA: 15:41 - loss: 2.8805 - acc: 0.08 - ETA: 15:35 - loss: 2.8804 - acc: 0.08 - ETA: 15:29 - loss: 2.8804 - acc: 0.08 - ETA: 15:23 - loss: 2.8804 - acc: 0.08 - ETA: 15:18 - loss: 2.8803 - acc: 0.08 - ETA: 15:12 - loss: 2.8803 - acc: 0.08 - ETA: 15:07 - loss: 2.8803 - acc: 0.08 - ETA: 15:02 - loss: 2.8803 - acc: 0.08 - ETA: 14:57 - loss: 2.8802 - acc: 0.08 - ETA: 14:52 - loss: 2.8802 - acc: 0.08 - ETA: 14:47 - loss: 2.8802 - acc: 0.08 - ETA: 14:43 - loss: 2.8802 - acc: 0.08 - ETA: 14:38 - loss: 2.8801 - acc: 0.08 - ETA: 14:34 - loss: 2.8801 - acc: 0.08 - ETA: 14:30 - loss: 2.8801 - acc: 0.08 - ETA: 14:26 - loss: 2.8800 - acc: 0.08 - ETA: 14:22 - loss: 2.8800 - acc: 0.08 - ETA: 14:18 - loss: 2.8800 - acc: 0.08 - ETA: 14:14 - loss: 2.8800 - acc: 0.08 - ETA: 14:10 - loss: 2.8799 - acc: 0.08 - ETA: 14:06 - loss: 2.8799 - acc: 0.08 - ETA: 14:03 - loss: 2.8799 - acc: 0.08 - ETA: 13:59 - loss: 2.8799 - acc: 0.08 - ETA: 13:56 - loss: 2.8798 - acc: 0.08 - ETA: 13:52 - loss: 2.8798 - acc: 0.08 - ETA: 13:49 - loss: 2.8798 - acc: 0.08 - ETA: 13:46 - loss: 2.8797 - acc: 0.08 - ETA: 13:43 - loss: 2.8797 - acc: 0.08 - ETA: 13:39 - loss: 2.8797 - acc: 0.08 - ETA: 13:36 - loss: 2.8797 - acc: 0.08 - ETA: 13:33 - loss: 2.8796 - acc: 0.08 - ETA: 13:30 - loss: 2.8796 - acc: 0.08 - ETA: 13:27 - loss: 2.8796 - acc: 0.08 - ETA: 13:25 - loss: 2.8795 - acc: 0.08 - ETA: 13:22 - loss: 2.8795 - acc: 0.08 - ETA: 13:19 - loss: 2.8795 - acc: 0.08 - ETA: 13:16 - loss: 2.8795 - acc: 0.08 - ETA: 13:14 - loss: 2.8794 - acc: 0.08 - ETA: 13:11 - loss: 2.8794 - acc: 0.08 - ETA: 13:09 - loss: 2.8794 - acc: 0.08 - ETA: 13:06 - loss: 2.8793 - acc: 0.08 - ETA: 13:03 - loss: 2.8793 - acc: 0.08 - ETA: 13:01 - loss: 2.8793 - acc: 0.08 - ETA: 12:58 - loss: 2.8793 - acc: 0.08 - ETA: 12:56 - loss: 2.8792 - acc: 0.08 - ETA: 12:54 - loss: 2.8792 - acc: 0.08 - ETA: 12:51 - loss: 2.8792 - acc: 0.08 - ETA: 12:49 - loss: 2.8791 - acc: 0.08 - ETA: 12:47 - loss: 2.8791 - acc: 0.08 - ETA: 12:45 - loss: 2.8791 - acc: 0.08 - ETA: 12:43 - loss: 2.8791 - acc: 0.08 - ETA: 12:40 - loss: 2.8790 - acc: 0.08 - ETA: 12:38 - loss: 2.8790 - acc: 0.08 - ETA: 12:36 - loss: 2.8790 - acc: 0.08 - ETA: 12:34 - loss: 2.8789 - acc: 0.08 - ETA: 12:32 - loss: 2.8789 - acc: 0.08 - ETA: 12:30 - loss: 2.8789 - acc: 0.08 - ETA: 12:28 - loss: 2.8789 - acc: 0.08 - ETA: 12:26 - loss: 2.8788 - acc: 0.08 - ETA: 12:24 - loss: 2.8788 - acc: 0.08 - ETA: 12:22 - loss: 2.8788 - acc: 0.08 - ETA: 12:20 - loss: 2.8788 - acc: 0.08 - ETA: 12:18 - loss: 2.8787 - acc: 0.08 - ETA: 12:16 - loss: 2.8787 - acc: 0.08 - ETA: 12:14 - loss: 2.8787 - acc: 0.08 - ETA: 12:13 - loss: 2.8786 - acc: 0.08 - ETA: 12:11 - loss: 2.8786 - acc: 0.08 - ETA: 12:09 - loss: 2.8786 - acc: 0.08 - ETA: 12:07 - loss: 2.8786 - acc: 0.08 - ETA: 12:05 - loss: 2.8785 - acc: 0.08 - ETA: 12:04 - loss: 2.8785 - acc: 0.08 - ETA: 12:02 - loss: 2.8785 - acc: 0.08 - ETA: 12:00 - loss: 2.8784 - acc: 0.08 - ETA: 11:59 - loss: 2.8784 - acc: 0.08 - ETA: 11:57 - loss: 2.8784 - acc: 0.08 - ETA: 11:55 - loss: 2.8784 - acc: 0.08 - ETA: 11:54 - loss: 2.8783 - acc: 0.08 - ETA: 11:52 - loss: 2.8783 - acc: 0.08 - ETA: 11:51 - loss: 2.8783 - acc: 0.08 - ETA: 11:49 - loss: 2.8782 - acc: 0.08 - ETA: 11:47 - loss: 2.8782 - acc: 0.08 - ETA: 11:46 - loss: 2.8782 - acc: 0.08 - ETA: 11:44 - loss: 2.8782 - acc: 0.08 - ETA: 11:43 - loss: 2.8781 - acc: 0.08 - ETA: 11:41 - loss: 2.8781 - acc: 0.08 - ETA: 11:40 - loss: 2.8781 - acc: 0.08 - ETA: 11:38 - loss: 2.8780 - acc: 0.08 - ETA: 11:37 - loss: 2.8780 - acc: 0.08 - ETA: 11:35 - loss: 2.8780 - acc: 0.08 - ETA: 11:34 - loss: 2.8780 - acc: 0.08 - ETA: 11:32 - loss: 2.8779 - acc: 0.08 - ETA: 11:31 - loss: 2.8779 - acc: 0.08 - ETA: 11:30 - loss: 2.8779 - acc: 0.08 - ETA: 11:28 - loss: 2.8778 - acc: 0.08 - ETA: 11:27 - loss: 2.8778 - acc: 0.08 - ETA: 11:25 - loss: 2.8778 - acc: 0.08 - ETA: 11:24 - loss: 2.8778 - acc: 0.08 - ETA: 11:23 - loss: 2.8777 - acc: 0.08 - ETA: 11:21 - loss: 2.8777 - acc: 0.08 - ETA: 11:20 - loss: 2.8777 - acc: 0.08 - ETA: 11:19 - loss: 2.8776 - acc: 0.08 - ETA: 11:17 - loss: 2.8776 - acc: 0.08 - ETA: 11:16 - loss: 2.8776 - acc: 0.08 - ETA: 11:15 - loss: 2.8776 - acc: 0.08 - ETA: 11:13 - loss: 2.8775 - acc: 0.08 - ETA: 11:12 - loss: 2.8775 - acc: 0.08 - ETA: 11:11 - loss: 2.8775 - acc: 0.08 - ETA: 11:09 - loss: 2.8774 - acc: 0.08 - ETA: 11:08 - loss: 2.8774 - acc: 0.08 - ETA: 11:07 - loss: 2.8774 - acc: 0.08 - ETA: 11:05 - loss: 2.8774 - acc: 0.08 - ETA: 11:04 - loss: 2.8773 - acc: 0.08 - ETA: 11:03 - loss: 2.8773 - acc: 0.08 - ETA: 11:02 - loss: 2.8773 - acc: 0.08 - ETA: 11:00 - loss: 2.8772 - acc: 0.08 - ETA: 10:59 - loss: 2.8772 - acc: 0.08 - ETA: 10:58 - loss: 2.8772 - acc: 0.08 - ETA: 10:57 - loss: 2.8772 - acc: 0.08 - ETA: 10:55 - loss: 2.8771 - acc: 0.08 - ETA: 10:54 - loss: 2.8771 - acc: 0.08 - ETA: 10:53 - loss: 2.8771 - acc: 0.08 - ETA: 10:52 - loss: 2.8770 - acc: 0.08 - ETA: 10:51 - loss: 2.8770 - acc: 0.08 - ETA: 10:49 - loss: 2.8770 - acc: 0.08 - ETA: 10:48 - loss: 2.8770 - acc: 0.08 - ETA: 10:47 - loss: 2.8769 - acc: 0.08 - ETA: 10:46 - loss: 2.8769 - acc: 0.08 - ETA: 10:45 - loss: 2.8769 - acc: 0.08 - ETA: 10:44 - loss: 2.8768 - acc: 0.08 - ETA: 10:42 - loss: 2.8768 - acc: 0.08 - ETA: 10:41 - loss: 2.8768 - acc: 0.08 - ETA: 10:40 - loss: 2.8768 - acc: 0.08 - ETA: 10:39 - loss: 2.8767 - acc: 0.08 - ETA: 10:38 - loss: 2.8767 - acc: 0.08 - ETA: 10:37 - loss: 2.8767 - acc: 0.08 - ETA: 10:36 - loss: 2.8766 - acc: 0.08 - ETA: 10:35 - loss: 2.8766 - acc: 0.08 - ETA: 10:34 - loss: 2.8766 - acc: 0.08 - ETA: 10:33 - loss: 2.8766 - acc: 0.08 - ETA: 10:31 - loss: 2.8765 - acc: 0.08 - ETA: 10:30 - loss: 2.8765 - acc: 0.08 - ETA: 10:29 - loss: 2.8765 - acc: 0.08 - ETA: 10:28 - loss: 2.8764 - acc: 0.08 - ETA: 10:27 - loss: 2.8764 - acc: 0.08 - ETA: 10:26 - loss: 2.8764 - acc: 0.08 - ETA: 10:25 - loss: 2.8764 - acc: 0.08 - ETA: 10:24 - loss: 2.8763 - acc: 0.08 - ETA: 10:23 - loss: 2.8763 - acc: 0.08 - ETA: 10:22 - loss: 2.8763 - acc: 0.0847 408/1000 [===========>..................] - ETA: 10:21 - loss: 2.8762 - acc: 0.08 - ETA: 10:36 - loss: 2.8762 - acc: 0.08 - ETA: 10:35 - loss: 2.8762 - acc: 0.08 - ETA: 10:34 - loss: 2.8762 - acc: 0.08 - ETA: 10:32 - loss: 2.8761 - acc: 0.08 - ETA: 10:31 - loss: 2.8761 - acc: 0.08 - ETA: 10:30 - loss: 2.8761 - acc: 0.08 - ETA: 10:29 - loss: 2.8760 - acc: 0.08 - ETA: 10:28 - loss: 2.8760 - acc: 0.08 - ETA: 10:27 - loss: 2.8760 - acc: 0.08 - ETA: 10:26 - loss: 2.8760 - acc: 0.08 - ETA: 10:25 - loss: 2.8759 - acc: 0.08 - ETA: 10:24 - loss: 2.8759 - acc: 0.08 - ETA: 10:22 - loss: 2.8759 - acc: 0.08 - ETA: 10:21 - loss: 2.8758 - acc: 0.08 - ETA: 10:20 - loss: 2.8758 - acc: 0.08 - ETA: 10:19 - loss: 2.8758 - acc: 0.08 - ETA: 10:18 - loss: 2.8758 - acc: 0.08 - ETA: 10:17 - loss: 2.8757 - acc: 0.08 - ETA: 10:16 - loss: 2.8757 - acc: 0.08 - ETA: 10:15 - loss: 2.8757 - acc: 0.08 - ETA: 10:14 - loss: 2.8756 - acc: 0.08 - ETA: 10:13 - loss: 2.8756 - acc: 0.08 - ETA: 10:12 - loss: 2.8756 - acc: 0.08 - ETA: 10:11 - loss: 2.8756 - acc: 0.08 - ETA: 10:10 - loss: 2.8755 - acc: 0.08 - ETA: 10:09 - loss: 2.8755 - acc: 0.08 - ETA: 10:08 - loss: 2.8755 - acc: 0.08 - ETA: 10:07 - loss: 2.8754 - acc: 0.08 - ETA: 10:05 - loss: 2.8754 - acc: 0.08 - ETA: 10:04 - loss: 2.8754 - acc: 0.08 - ETA: 10:03 - loss: 2.8754 - acc: 0.08 - ETA: 10:02 - loss: 2.8753 - acc: 0.08 - ETA: 10:01 - loss: 2.8753 - acc: 0.08 - ETA: 10:00 - loss: 2.8753 - acc: 0.08 - ETA: 9:59 - loss: 2.8752 - acc: 0.0848 - ETA: 9:58 - loss: 2.8752 - acc: 0.084 - ETA: 9:57 - loss: 2.8752 - acc: 0.084 - ETA: 9:56 - loss: 2.8752 - acc: 0.084 - ETA: 9:55 - loss: 2.8751 - acc: 0.084 - ETA: 9:54 - loss: 2.8751 - acc: 0.084 - ETA: 9:53 - loss: 2.8751 - acc: 0.084 - ETA: 9:52 - loss: 2.8750 - acc: 0.084 - ETA: 9:51 - loss: 2.8750 - acc: 0.084 - ETA: 9:50 - loss: 2.8750 - acc: 0.084 - ETA: 9:49 - loss: 2.8750 - acc: 0.084 - ETA: 9:48 - loss: 2.8749 - acc: 0.084 - ETA: 9:47 - loss: 2.8749 - acc: 0.084 - ETA: 9:46 - loss: 2.8749 - acc: 0.084 - ETA: 9:45 - loss: 2.8748 - acc: 0.084 - ETA: 9:44 - loss: 2.8748 - acc: 0.084 - ETA: 9:43 - loss: 2.8748 - acc: 0.084 - ETA: 9:42 - loss: 2.8747 - acc: 0.084 - ETA: 9:41 - loss: 2.8747 - acc: 0.084 - ETA: 9:40 - loss: 2.8747 - acc: 0.084 - ETA: 9:39 - loss: 2.8747 - acc: 0.084 - ETA: 9:38 - loss: 2.8746 - acc: 0.084 - ETA: 9:37 - loss: 2.8746 - acc: 0.084 - ETA: 9:36 - loss: 2.8746 - acc: 0.084 - ETA: 9:35 - loss: 2.8745 - acc: 0.084 - ETA: 9:34 - loss: 2.8745 - acc: 0.084 - ETA: 9:34 - loss: 2.8745 - acc: 0.084 - ETA: 9:33 - loss: 2.8745 - acc: 0.084 - ETA: 9:32 - loss: 2.8744 - acc: 0.084 - ETA: 9:31 - loss: 2.8744 - acc: 0.084 - ETA: 9:30 - loss: 2.8744 - acc: 0.084 - ETA: 9:29 - loss: 2.8743 - acc: 0.084 - ETA: 9:28 - loss: 2.8743 - acc: 0.084 - ETA: 9:27 - loss: 2.8743 - acc: 0.084 - ETA: 9:26 - loss: 2.8743 - acc: 0.084 - ETA: 9:25 - loss: 2.8742 - acc: 0.084 - ETA: 9:24 - loss: 2.8742 - acc: 0.084 - ETA: 9:23 - loss: 2.8742 - acc: 0.084 - ETA: 9:22 - loss: 2.8741 - acc: 0.084 - ETA: 9:21 - loss: 2.8741 - acc: 0.085 - ETA: 9:20 - loss: 2.8741 - acc: 0.085 - ETA: 9:19 - loss: 2.8741 - acc: 0.085 - ETA: 9:18 - loss: 2.8740 - acc: 0.085 - ETA: 9:18 - loss: 2.8740 - acc: 0.085 - ETA: 9:17 - loss: 2.8740 - acc: 0.085 - ETA: 9:16 - loss: 2.8739 - acc: 0.085 - ETA: 9:15 - loss: 2.8739 - acc: 0.085 - ETA: 9:14 - loss: 2.8739 - acc: 0.085 - ETA: 9:13 - loss: 2.8739 - acc: 0.085 - ETA: 9:12 - loss: 2.8738 - acc: 0.085 - ETA: 9:11 - loss: 2.8738 - acc: 0.085 - ETA: 9:10 - loss: 2.8738 - acc: 0.085 - ETA: 9:09 - loss: 2.8737 - acc: 0.085 - ETA: 9:08 - loss: 2.8737 - acc: 0.085 - ETA: 9:07 - loss: 2.8737 - acc: 0.085 - ETA: 9:06 - loss: 2.8737 - acc: 0.085 - ETA: 9:06 - loss: 2.8736 - acc: 0.084 - ETA: 9:05 - loss: 2.8736 - acc: 0.084 - ETA: 9:04 - loss: 2.8736 - acc: 0.084 - ETA: 9:03 - loss: 2.8735 - acc: 0.085 - ETA: 9:02 - loss: 2.8735 - acc: 0.085 - ETA: 9:01 - loss: 2.8735 - acc: 0.085 - ETA: 9:00 - loss: 2.8735 - acc: 0.085 - ETA: 8:59 - loss: 2.8734 - acc: 0.085 - ETA: 8:58 - loss: 2.8734 - acc: 0.084 - ETA: 8:57 - loss: 2.8734 - acc: 0.084 - ETA: 8:56 - loss: 2.8733 - acc: 0.084 - ETA: 8:56 - loss: 2.8733 - acc: 0.084 - ETA: 8:55 - loss: 2.8733 - acc: 0.084 - ETA: 8:54 - loss: 2.8733 - acc: 0.084 - ETA: 8:53 - loss: 2.8732 - acc: 0.084 - ETA: 8:52 - loss: 2.8732 - acc: 0.084 - ETA: 8:51 - loss: 2.8732 - acc: 0.084 - ETA: 8:50 - loss: 2.8731 - acc: 0.084 - ETA: 8:49 - loss: 2.8731 - acc: 0.084 - ETA: 8:48 - loss: 2.8731 - acc: 0.084 - ETA: 8:48 - loss: 2.8731 - acc: 0.084 - ETA: 8:47 - loss: 2.8730 - acc: 0.084 - ETA: 8:46 - loss: 2.8730 - acc: 0.084 - ETA: 8:45 - loss: 2.8730 - acc: 0.084 - ETA: 8:44 - loss: 2.8729 - acc: 0.085 - ETA: 8:43 - loss: 2.8729 - acc: 0.084 - ETA: 8:42 - loss: 2.8729 - acc: 0.084 - ETA: 8:42 - loss: 2.8729 - acc: 0.084 - ETA: 8:41 - loss: 2.8728 - acc: 0.084 - ETA: 8:40 - loss: 2.8728 - acc: 0.084 - ETA: 8:39 - loss: 2.8728 - acc: 0.084 - ETA: 8:38 - loss: 2.8727 - acc: 0.084 - ETA: 8:37 - loss: 2.8727 - acc: 0.085 - ETA: 8:36 - loss: 2.8727 - acc: 0.084 - ETA: 8:36 - loss: 2.8727 - acc: 0.084 - ETA: 8:35 - loss: 2.8726 - acc: 0.084 - ETA: 8:34 - loss: 2.8726 - acc: 0.084 - ETA: 8:33 - loss: 2.8726 - acc: 0.084 - ETA: 8:32 - loss: 2.8725 - acc: 0.084 - ETA: 8:31 - loss: 2.8725 - acc: 0.085 - ETA: 8:30 - loss: 2.8725 - acc: 0.085 - ETA: 8:30 - loss: 2.8725 - acc: 0.085 - ETA: 8:29 - loss: 2.8724 - acc: 0.085 - ETA: 8:28 - loss: 2.8724 - acc: 0.085 - ETA: 8:27 - loss: 2.8724 - acc: 0.085 - ETA: 8:26 - loss: 2.8723 - acc: 0.085 - ETA: 8:25 - loss: 2.8723 - acc: 0.085 - ETA: 8:25 - loss: 2.8723 - acc: 0.085 - ETA: 8:24 - loss: 2.8723 - acc: 0.085 - ETA: 8:23 - loss: 2.8722 - acc: 0.085 - ETA: 8:22 - loss: 2.8722 - acc: 0.085 - ETA: 8:21 - loss: 2.8722 - acc: 0.085 - ETA: 8:20 - loss: 2.8721 - acc: 0.085 - ETA: 8:19 - loss: 2.8721 - acc: 0.085 - ETA: 8:18 - loss: 2.8721 - acc: 0.085 - ETA: 8:18 - loss: 2.8721 - acc: 0.085 - ETA: 8:17 - loss: 2.8720 - acc: 0.085 - ETA: 8:16 - loss: 2.8720 - acc: 0.085 - ETA: 8:15 - loss: 2.8720 - acc: 0.085 - ETA: 8:14 - loss: 2.8719 - acc: 0.085 - ETA: 8:13 - loss: 2.8719 - acc: 0.085 - ETA: 8:13 - loss: 2.8719 - acc: 0.085 - ETA: 8:12 - loss: 2.8719 - acc: 0.085 - ETA: 8:11 - loss: 2.8718 - acc: 0.085 - ETA: 8:10 - loss: 2.8718 - acc: 0.085 - ETA: 8:09 - loss: 2.8718 - acc: 0.085 - ETA: 8:08 - loss: 2.8717 - acc: 0.084 - ETA: 8:08 - loss: 2.8717 - acc: 0.085 - ETA: 8:07 - loss: 2.8717 - acc: 0.085 - ETA: 8:06 - loss: 2.8717 - acc: 0.085 - ETA: 8:05 - loss: 2.8716 - acc: 0.085 - ETA: 8:04 - loss: 2.8716 - acc: 0.085 - ETA: 8:03 - loss: 2.8716 - acc: 0.085 - ETA: 8:03 - loss: 2.8715 - acc: 0.085 - ETA: 8:02 - loss: 2.8715 - acc: 0.085 - ETA: 8:01 - loss: 2.8715 - acc: 0.085 - ETA: 8:00 - loss: 2.8715 - acc: 0.085 - ETA: 7:59 - loss: 2.8714 - acc: 0.085 - ETA: 7:58 - loss: 2.8714 - acc: 0.085 - ETA: 7:58 - loss: 2.8714 - acc: 0.085 - ETA: 7:57 - loss: 2.8713 - acc: 0.085 - ETA: 7:56 - loss: 2.8713 - acc: 0.085 - ETA: 7:55 - loss: 2.8713 - acc: 0.085 - ETA: 7:54 - loss: 2.8713 - acc: 0.085 - ETA: 7:53 - loss: 2.8712 - acc: 0.085 - ETA: 7:52 - loss: 2.8712 - acc: 0.085 - ETA: 7:52 - loss: 2.8712 - acc: 0.085 - ETA: 7:51 - loss: 2.8711 - acc: 0.085 - ETA: 7:50 - loss: 2.8711 - acc: 0.085 - ETA: 7:49 - loss: 2.8711 - acc: 0.085 - ETA: 7:48 - loss: 2.8711 - acc: 0.085 - ETA: 7:47 - loss: 2.8710 - acc: 0.085 - ETA: 7:47 - loss: 2.8710 - acc: 0.085 - ETA: 7:46 - loss: 2.8710 - acc: 0.085 - ETA: 7:45 - loss: 2.8709 - acc: 0.085 - ETA: 7:44 - loss: 2.8709 - acc: 0.085 - ETA: 7:43 - loss: 2.8709 - acc: 0.085 - ETA: 7:42 - loss: 2.8709 - acc: 0.085 - ETA: 7:42 - loss: 2.8708 - acc: 0.085 - ETA: 7:41 - loss: 2.8708 - acc: 0.085 - ETA: 7:40 - loss: 2.8708 - acc: 0.085 - ETA: 7:39 - loss: 2.8707 - acc: 0.085 - ETA: 7:38 - loss: 2.8707 - acc: 0.085 - ETA: 7:38 - loss: 2.8707 - acc: 0.085 - ETA: 7:37 - loss: 2.8707 - acc: 0.085 - ETA: 7:36 - loss: 2.8706 - acc: 0.085 - ETA: 7:35 - loss: 2.8706 - acc: 0.085 - ETA: 7:34 - loss: 2.8706 - acc: 0.085 - ETA: 7:34 - loss: 2.8705 - acc: 0.085 - ETA: 7:33 - loss: 2.8705 - acc: 0.085 - ETA: 7:32 - loss: 2.8705 - acc: 0.085 - ETA: 7:31 - loss: 2.8705 - acc: 0.085 - ETA: 7:30 - loss: 2.8704 - acc: 0.0850 571/1000 [================>.............] - ETA: 7:30 - loss: 2.8704 - acc: 0.085 - ETA: 7:29 - loss: 2.8704 - acc: 0.085 - ETA: 7:28 - loss: 2.8703 - acc: 0.085 - ETA: 7:27 - loss: 2.8703 - acc: 0.085 - ETA: 7:26 - loss: 2.8703 - acc: 0.085 - ETA: 7:25 - loss: 2.8703 - acc: 0.085 - ETA: 7:25 - loss: 2.8702 - acc: 0.085 - ETA: 7:24 - loss: 2.8702 - acc: 0.085 - ETA: 7:23 - loss: 2.8702 - acc: 0.085 - ETA: 7:22 - loss: 2.8701 - acc: 0.085 - ETA: 7:21 - loss: 2.8701 - acc: 0.085 - ETA: 7:21 - loss: 2.8701 - acc: 0.085 - ETA: 7:20 - loss: 2.8701 - acc: 0.085 - ETA: 7:19 - loss: 2.8700 - acc: 0.085 - ETA: 7:18 - loss: 2.8700 - acc: 0.085 - ETA: 7:17 - loss: 2.8700 - acc: 0.085 - ETA: 7:17 - loss: 2.8699 - acc: 0.085 - ETA: 7:16 - loss: 2.8699 - acc: 0.085 - ETA: 7:15 - loss: 2.8699 - acc: 0.085 - ETA: 7:14 - loss: 2.8699 - acc: 0.085 - ETA: 7:13 - loss: 2.8698 - acc: 0.085 - ETA: 7:12 - loss: 2.8698 - acc: 0.085 - ETA: 7:12 - loss: 2.8698 - acc: 0.085 - ETA: 7:11 - loss: 2.8697 - acc: 0.085 - ETA: 7:10 - loss: 2.8697 - acc: 0.085 - ETA: 7:09 - loss: 2.8697 - acc: 0.085 - ETA: 7:09 - loss: 2.8697 - acc: 0.085 - ETA: 7:08 - loss: 2.8696 - acc: 0.085 - ETA: 7:07 - loss: 2.8696 - acc: 0.085 - ETA: 7:06 - loss: 2.8696 - acc: 0.085 - ETA: 7:05 - loss: 2.8695 - acc: 0.085 - ETA: 7:05 - loss: 2.8695 - acc: 0.085 - ETA: 7:04 - loss: 2.8695 - acc: 0.085 - ETA: 7:03 - loss: 2.8695 - acc: 0.085 - ETA: 7:02 - loss: 2.8694 - acc: 0.085 - ETA: 7:01 - loss: 2.8694 - acc: 0.085 - ETA: 7:00 - loss: 2.8694 - acc: 0.085 - ETA: 7:00 - loss: 2.8693 - acc: 0.085 - ETA: 6:59 - loss: 2.8693 - acc: 0.085 - ETA: 6:58 - loss: 2.8693 - acc: 0.085 - ETA: 6:57 - loss: 2.8693 - acc: 0.085 - ETA: 6:56 - loss: 2.8692 - acc: 0.085 - ETA: 6:56 - loss: 2.8692 - acc: 0.085 - ETA: 6:55 - loss: 2.8692 - acc: 0.085 - ETA: 6:54 - loss: 2.8691 - acc: 0.085 - ETA: 6:53 - loss: 2.8691 - acc: 0.085 - ETA: 6:52 - loss: 2.8691 - acc: 0.085 - ETA: 6:52 - loss: 2.8690 - acc: 0.085 - ETA: 6:51 - loss: 2.8690 - acc: 0.085 - ETA: 6:50 - loss: 2.8690 - acc: 0.085 - ETA: 6:49 - loss: 2.8690 - acc: 0.085 - ETA: 6:49 - loss: 2.8689 - acc: 0.085 - ETA: 6:48 - loss: 2.8689 - acc: 0.085 - ETA: 6:47 - loss: 2.8689 - acc: 0.085 - ETA: 6:46 - loss: 2.8688 - acc: 0.085 - ETA: 6:45 - loss: 2.8688 - acc: 0.085 - ETA: 6:45 - loss: 2.8688 - acc: 0.085 - ETA: 6:44 - loss: 2.8688 - acc: 0.085 - ETA: 6:43 - loss: 2.8687 - acc: 0.085 - ETA: 6:42 - loss: 2.8687 - acc: 0.085 - ETA: 6:41 - loss: 2.8687 - acc: 0.085 - ETA: 6:41 - loss: 2.8686 - acc: 0.085 - ETA: 6:40 - loss: 2.8686 - acc: 0.085 - ETA: 6:39 - loss: 2.8686 - acc: 0.085 - ETA: 6:38 - loss: 2.8686 - acc: 0.085 - ETA: 6:37 - loss: 2.8685 - acc: 0.085 - ETA: 6:37 - loss: 2.8685 - acc: 0.085 - ETA: 6:36 - loss: 2.8685 - acc: 0.085 - ETA: 6:35 - loss: 2.8684 - acc: 0.085 - ETA: 6:34 - loss: 2.8684 - acc: 0.085 - ETA: 6:34 - loss: 2.8684 - acc: 0.085 - ETA: 6:33 - loss: 2.8684 - acc: 0.085 - ETA: 6:32 - loss: 2.8683 - acc: 0.085 - ETA: 6:31 - loss: 2.8683 - acc: 0.085 - ETA: 6:30 - loss: 2.8683 - acc: 0.085 - ETA: 6:30 - loss: 2.8682 - acc: 0.085 - ETA: 6:29 - loss: 2.8682 - acc: 0.085 - ETA: 6:28 - loss: 2.8682 - acc: 0.085 - ETA: 6:27 - loss: 2.8682 - acc: 0.085 - ETA: 6:26 - loss: 2.8681 - acc: 0.085 - ETA: 6:26 - loss: 2.8681 - acc: 0.085 - ETA: 6:25 - loss: 2.8681 - acc: 0.085 - ETA: 6:24 - loss: 2.8680 - acc: 0.085 - ETA: 6:23 - loss: 2.8680 - acc: 0.085 - ETA: 6:22 - loss: 2.8680 - acc: 0.085 - ETA: 6:22 - loss: 2.8680 - acc: 0.085 - ETA: 6:21 - loss: 2.8679 - acc: 0.085 - ETA: 6:20 - loss: 2.8679 - acc: 0.085 - ETA: 6:19 - loss: 2.8679 - acc: 0.085 - ETA: 6:19 - loss: 2.8678 - acc: 0.085 - ETA: 6:18 - loss: 2.8678 - acc: 0.085 - ETA: 6:17 - loss: 2.8678 - acc: 0.085 - ETA: 6:16 - loss: 2.8678 - acc: 0.085 - ETA: 6:15 - loss: 2.8677 - acc: 0.085 - ETA: 6:15 - loss: 2.8677 - acc: 0.085 - ETA: 6:14 - loss: 2.8677 - acc: 0.085 - ETA: 6:13 - loss: 2.8676 - acc: 0.085 - ETA: 6:12 - loss: 2.8676 - acc: 0.085 - ETA: 6:11 - loss: 2.8676 - acc: 0.085 - ETA: 6:11 - loss: 2.8676 - acc: 0.085 - ETA: 6:10 - loss: 2.8675 - acc: 0.085 - ETA: 6:09 - loss: 2.8675 - acc: 0.085 - ETA: 6:08 - loss: 2.8675 - acc: 0.085 - ETA: 6:08 - loss: 2.8674 - acc: 0.085 - ETA: 6:07 - loss: 2.8674 - acc: 0.085 - ETA: 6:06 - loss: 2.8674 - acc: 0.085 - ETA: 6:05 - loss: 2.8674 - acc: 0.085 - ETA: 6:04 - loss: 2.8673 - acc: 0.085 - ETA: 6:04 - loss: 2.8673 - acc: 0.085 - ETA: 6:03 - loss: 2.8673 - acc: 0.085 - ETA: 6:02 - loss: 2.8672 - acc: 0.085 - ETA: 6:01 - loss: 2.8672 - acc: 0.085 - ETA: 6:01 - loss: 2.8672 - acc: 0.085 - ETA: 6:00 - loss: 2.8672 - acc: 0.085 - ETA: 5:59 - loss: 2.8671 - acc: 0.085 - ETA: 5:58 - loss: 2.8671 - acc: 0.085 - ETA: 5:57 - loss: 2.8671 - acc: 0.085 - ETA: 5:57 - loss: 2.8670 - acc: 0.085 - ETA: 5:56 - loss: 2.8670 - acc: 0.085 - ETA: 5:55 - loss: 2.8670 - acc: 0.085 - ETA: 5:54 - loss: 2.8670 - acc: 0.085 - ETA: 5:54 - loss: 2.8669 - acc: 0.085 - ETA: 5:53 - loss: 2.8669 - acc: 0.085 - ETA: 5:52 - loss: 2.8669 - acc: 0.085 - ETA: 5:51 - loss: 2.8668 - acc: 0.085 - ETA: 5:50 - loss: 2.8668 - acc: 0.085 - ETA: 5:50 - loss: 2.8668 - acc: 0.085 - ETA: 5:49 - loss: 2.8668 - acc: 0.085 - ETA: 5:48 - loss: 2.8667 - acc: 0.085 - ETA: 5:47 - loss: 2.8667 - acc: 0.085 - ETA: 5:47 - loss: 2.8667 - acc: 0.085 - ETA: 5:46 - loss: 2.8666 - acc: 0.085 - ETA: 5:45 - loss: 2.8666 - acc: 0.085 - ETA: 5:44 - loss: 2.8666 - acc: 0.085 - ETA: 5:44 - loss: 2.8666 - acc: 0.085 - ETA: 5:43 - loss: 2.8665 - acc: 0.085 - ETA: 5:42 - loss: 2.8665 - acc: 0.085 - ETA: 5:41 - loss: 2.8665 - acc: 0.085 - ETA: 5:40 - loss: 2.8664 - acc: 0.085 - ETA: 5:40 - loss: 2.8664 - acc: 0.085 - ETA: 5:39 - loss: 2.8664 - acc: 0.085 - ETA: 5:38 - loss: 2.8664 - acc: 0.085 - ETA: 5:37 - loss: 2.8663 - acc: 0.085 - ETA: 5:37 - loss: 2.8663 - acc: 0.085 - ETA: 5:36 - loss: 2.8663 - acc: 0.085 - ETA: 5:35 - loss: 2.8662 - acc: 0.085 - ETA: 5:34 - loss: 2.8662 - acc: 0.085 - ETA: 5:34 - loss: 2.8662 - acc: 0.085 - ETA: 5:33 - loss: 2.8662 - acc: 0.085 - ETA: 5:32 - loss: 2.8661 - acc: 0.085 - ETA: 5:31 - loss: 2.8661 - acc: 0.085 - ETA: 5:30 - loss: 2.8661 - acc: 0.085 - ETA: 5:30 - loss: 2.8660 - acc: 0.085 - ETA: 5:29 - loss: 2.8660 - acc: 0.085 - ETA: 5:28 - loss: 2.8660 - acc: 0.085 - ETA: 5:27 - loss: 2.8660 - acc: 0.085 - ETA: 5:27 - loss: 2.8659 - acc: 0.085 - ETA: 5:26 - loss: 2.8659 - acc: 0.085 - ETA: 5:25 - loss: 2.8659 - acc: 0.085 - ETA: 5:24 - loss: 2.8658 - acc: 0.085 - ETA: 5:24 - loss: 2.8658 - acc: 0.085 - ETA: 5:23 - loss: 2.8658 - acc: 0.085 - ETA: 5:22 - loss: 2.8658 - acc: 0.0850"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f9416c0c56f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfinal_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#### If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 30\n",
    "steps_per_epoch = 300\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              epochs=final_epoch,\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
